{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Project Team 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Load data set in pandas data frame\n",
    "df = pd.read_csv('bankruptcy_Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Attr1     Attr2     Attr3     Attr4     Attr5     Attr6     Attr7  \\\n",
      "0 -0.031545 -0.091313 -0.040269 -0.013529  0.007406 -0.016047 -0.000264   \n",
      "1 -0.231729 -0.049448  0.304381 -0.080975  0.007515 -0.016047 -0.034963   \n",
      "2 -0.058602  0.065060 -0.488404 -0.189489  0.006572 -0.016047 -0.004954   \n",
      "3 -0.069376  0.044641 -0.181684 -0.140032  0.007477 -0.010915 -0.005599   \n",
      "4  0.236424 -0.051912  0.678337 -0.014680  0.007879 -0.016047  0.057418   \n",
      "5  0.133748 -0.047473  0.262848 -0.090748  0.007288  0.058120  0.038096   \n",
      "6  0.102177 -0.031209  1.129460  0.002407  0.008103  0.042720  0.031246   \n",
      "7 -0.198342  0.135906 -0.011195 -0.151052  0.006729 -0.449586 -0.030718   \n",
      "8 -0.111029  0.152887 -1.540984 -0.280303  0.006049 -0.021231 -0.013745   \n",
      "9  0.272102  0.000103  0.814117 -0.065289  0.007522 -0.016047  0.052368   \n",
      "\n",
      "      Attr8     Attr9    Attr10  ...    Attr56    Attr57    Attr58     Attr59  \\\n",
      "0  0.641242 -0.748385  0.126789  ...  0.014367  0.005457 -0.014143  -0.020924   \n",
      "1  0.074710  0.469815  0.073759  ...  0.008492 -0.008385 -0.008666  -0.023095   \n",
      "2 -0.456287  0.270351 -0.071287  ...  0.010819  0.006779 -0.009437  -0.007919   \n",
      "3 -0.462971 -0.286746 -0.085266  ...  0.010683  0.005384 -0.010840   0.001381   \n",
      "4  0.097183  0.423405  0.076880  ...  0.010970  0.025295 -0.011056  -0.022535   \n",
      "5  0.057334 -0.303533  0.071257  ...  0.010477  0.018285 -0.010634  -0.023331   \n",
      "6 -0.118561 -0.345813  0.028659  ...  0.009924  0.018752 -0.010081  -0.022915   \n",
      "7 -0.587595  0.034352 -0.161029  ...  0.008196 -0.093773 -0.008612   0.085775   \n",
      "8 -0.610577 -0.336298 -0.182535  ...  0.010052 -1.884445 -0.010209  11.969653   \n",
      "9 -0.241358  0.512006  0.010994  ...  0.010708  0.036584 -0.010850  -0.023168   \n",
      "\n",
      "     Attr60    Attr61    Attr62    Attr63    Attr64  class  \n",
      "0  0.068399 -0.214478 -0.013915 -0.173939 -0.046788      0  \n",
      "1 -0.033498 -0.205796 -0.015174 -0.073056 -0.027236      0  \n",
      "2 -0.043455  0.019740 -0.011736 -0.291624 -0.033580      0  \n",
      "3 -0.042828 -0.350519  0.002969 -0.554685 -0.046823      0  \n",
      "4 -0.035892 -0.181557 -0.015623 -0.027841 -0.023694      0  \n",
      "5 -0.042159 -0.156714 -0.014385 -0.140025 -0.028525      0  \n",
      "6 -0.021995 -0.091241 -0.015119 -0.078196  0.022029      0  \n",
      "7 -0.045042 -0.180100 -0.006748 -0.436677 -0.022634      0  \n",
      "8 -0.042269  1.275610 -0.009024 -0.384045 -0.038609      0  \n",
      "9 -0.042553 -0.257159 -0.012451 -0.258756  0.023500      0  \n",
      "\n",
      "[10 rows x 65 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load data set in pandas data frame\n",
    "df = pd.read_csv('bankruptcy_train.csv')\n",
    "# Print first 10 rows to get a feel of the data\n",
    "print(df.head(10))\n",
    "dfsample1 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration - Predictor Selection\n",
    "\n",
    "### Checking for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attr1     0\n",
       "Attr2     0\n",
       "Attr3     0\n",
       "Attr4     0\n",
       "Attr5     0\n",
       "         ..\n",
       "Attr61    0\n",
       "Attr62    0\n",
       "Attr63    0\n",
       "Attr64    0\n",
       "class     0\n",
       "Length: 65, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking for any missing values\n",
    "dfsample1.isnull().sum()\n",
    "\n",
    "#No missing values identified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attr1 Attr6\n",
      "Attr1 Attr10\n",
      "Attr1 Attr25\n",
      "Attr1 Attr38\n",
      "Attr2 Attr7\n",
      "Attr2 Attr14\n",
      "Attr2 Attr18\n",
      "Attr4 Attr40\n",
      "Attr4 Attr46\n",
      "Attr6 Attr10\n",
      "Attr6 Attr25\n",
      "Attr6 Attr38\n",
      "Attr7 Attr11\n",
      "Attr7 Attr14\n",
      "Attr7 Attr18\n",
      "Attr7 Attr22\n",
      "Attr8 Attr17\n",
      "Attr10 Attr25\n",
      "Attr10 Attr38\n",
      "Attr11 Attr14\n",
      "Attr11 Attr18\n",
      "Attr11 Attr22\n",
      "Attr11 Attr35\n",
      "Attr11 Attr48\n",
      "Attr14 Attr18\n",
      "Attr14 Attr22\n",
      "Attr16 Attr26\n",
      "Attr18 Attr22\n",
      "Attr19 Attr23\n",
      "Attr19 Attr31\n",
      "Attr19 Attr39\n",
      "Attr19 Attr42\n",
      "Attr19 Attr49\n",
      "Attr19 Attr56\n",
      "Attr20 Attr43\n",
      "Attr22 Attr35\n",
      "Attr22 Attr48\n",
      "Attr23 Attr31\n",
      "Attr23 Attr39\n",
      "Attr23 Attr42\n",
      "Attr23 Attr49\n",
      "Attr23 Attr56\n",
      "Attr25 Attr38\n",
      "Attr28 Attr54\n",
      "Attr30 Attr43\n",
      "Attr30 Attr44\n",
      "Attr30 Attr62\n",
      "Attr31 Attr39\n",
      "Attr31 Attr42\n",
      "Attr31 Attr49\n",
      "Attr31 Attr56\n",
      "Attr32 Attr47\n",
      "Attr32 Attr52\n",
      "Attr33 Attr63\n",
      "Attr35 Attr48\n",
      "Attr39 Attr42\n",
      "Attr39 Attr49\n",
      "Attr39 Attr56\n",
      "Attr40 Attr46\n",
      "Attr42 Attr49\n",
      "Attr42 Attr56\n",
      "Attr43 Attr44\n",
      "Attr47 Attr52\n",
      "Attr49 Attr56\n",
      "Attr58 Attr62\n"
     ]
    }
   ],
   "source": [
    "# Data Exploration - Mean analysis:\n",
    "# Data Partition into Bankrupt and non-bankrupt Data sets: \n",
    "dfsample2 = dfsample1.copy()\n",
    "dfsample_brupt = dfsample2[dfsample2['class']==1]\n",
    "#dfsample_brupt\n",
    "dfsample_notbrupt = dfsample2[dfsample2['class']==0]\n",
    "#print(dfsample_notbrupt)\n",
    "\n",
    "\n",
    "#Calculating average for bankrupt data set:\n",
    "\n",
    "dfsample_brupt_1 = dfsample_brupt.copy()\n",
    "dfsample_bruptavg =pd.DataFrame(dfsample_brupt_1.mean())\n",
    "#print(dfsample_bruptavg)\n",
    "\n",
    "# Calculating average for non bankrupt Data set:\n",
    "dfsample_notbrupt_1 = dfsample_notbrupt.copy()\n",
    "dfsample_notbruptavg= pd.DataFrame(dfsample_notbrupt_1.mean())\n",
    "#print(dfsample_notbruptavg)\n",
    "\n",
    "\n",
    "# Calculating average for complete Data set:\n",
    "df_avgmean = pd.DataFrame(dfsample1.mean())\n",
    "#df_avgmean\n",
    "\n",
    "#Merging Combined and non bankrupt Data set: \n",
    "df_comb = pd.merge(df_avgmean,dfsample_notbruptavg,right_index=True,left_index=True,how='inner')\n",
    "#print(df_comb)\n",
    "\n",
    "# Merging bankrupt Data set : \n",
    "df_comb1 = pd.merge(df_comb,dfsample_bruptavg,right_index=True,left_index=True,how='inner')\n",
    "df_comb1.columns = [\"comb_means\",\"non_bankrupt\",\"bankrupt\"]\n",
    "#print(df_comb1)\n",
    "\n",
    "# Calculating Class dispersion between Bankrupt and Non Bankrupt Data set :\n",
    "\n",
    "df_comb1['Class_disp']=df_comb1['non_bankrupt']-df_comb1['bankrupt']\n",
    "df_comb2 = df_comb1.copy()\n",
    "df_comb3 = df_comb2.copy()\n",
    "df_comb3 = df_comb2.drop(index='class')\n",
    "#print(df_comb3)\n",
    "\n",
    "\n",
    "#Calculting Absolute value to sort attributes that make a difference between bankrupt and non bankrupt Data set:\n",
    "df_comb3['absolute value']= df_comb3['Class_disp'].abs()\n",
    "df_comb3\n",
    "\n",
    "# Attributes in Descending order of significant dispersion between Data sets  : \n",
    "\n",
    "df_comb4 = df_comb3.copy()\n",
    "df_comb4 = df_comb3.sort_values(by=['absolute value'],ascending=False)\n",
    "df_comb4.head(10)\n",
    "\n",
    "# Calculating correlation between each attribute and printing only the pairs that correlated greater than equal to +0.80:\n",
    "column_list = dfsample2.columns\n",
    "for i in range(0,len(column_list)-1):\n",
    "    for j in range(i+1,len(column_list)):\n",
    "        col_cor = dfsample2[column_list[i]].corr(dfsample2[column_list[j]])\n",
    "        if col_cor>=0.8:\n",
    "            print(column_list[i],column_list[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Attr2     Attr3     Attr5     Attr6     Attr9    Attr11    Attr12  \\\n",
      "0    -0.091313 -0.040269  0.007406 -0.016047 -0.748385 -0.214423 -0.036576   \n",
      "1    -0.049448  0.304381  0.007515 -0.016047  0.469815 -0.835505 -0.299606   \n",
      "2     0.065060 -0.488404  0.006572 -0.016047  0.270351  0.156944 -0.151208   \n",
      "3     0.044641 -0.181684  0.007477 -0.010915 -0.286746 -0.314756 -0.145529   \n",
      "4    -0.051912  0.678337  0.007879 -0.016047  0.423405  0.918860  0.208686   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "9995  0.034814 -0.492082  0.006687 -0.006462 -0.372026 -0.370083 -0.165392   \n",
      "9996 -0.095260  0.184167  0.007497 -0.034968 -0.393121 -0.390615 -0.169328   \n",
      "9997  0.061341 -0.830634  0.006716 -0.013742 -0.351828 -0.993696 -0.287411   \n",
      "9998  0.029524  0.102420  0.008123 -0.018374 -0.480887 -0.242626 -0.204565   \n",
      "9999 -0.081793  0.734155  0.007850  0.001952 -0.398417 -0.318317 -0.142172   \n",
      "\n",
      "        Attr13    Attr15    Attr16  ...    Attr53    Attr54    Attr55  \\\n",
      "0    -0.010930 -0.005305 -0.242796  ... -0.016363 -0.018113 -0.110578   \n",
      "1    -0.013057 -0.119627 -0.603332  ... -0.003012 -0.014462 -0.114919   \n",
      "2    -0.011717  0.009484 -0.367159  ... -0.030162 -0.019637 -0.123266   \n",
      "3    -0.011006  0.045912 -0.454498  ... -0.033926 -0.019978 -0.077078   \n",
      "4    -0.008905 -0.021015  0.744187  ...  0.005230 -0.010739 -0.076644   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "9995 -0.012080  0.081838 -0.482695  ... -0.025083 -0.019715 -0.123270   \n",
      "9996 -0.010569 -0.018260  0.270885  ... -0.012842 -0.017214  0.047160   \n",
      "9997 -0.014616 -0.059516 -0.717923  ... -0.033106 -0.022124 -0.323871   \n",
      "9998 -0.011467  0.021498 -0.410858  ... -0.028163 -0.017186 -0.105989   \n",
      "9999 -0.012127 -0.012039 -0.097137  ... -0.002851 -0.014428  0.151937   \n",
      "\n",
      "        Attr57    Attr59    Attr60    Attr61    Attr63    Attr64  class_1  \n",
      "0     0.005457 -0.020924  0.068399 -0.214478 -0.173939 -0.046788        0  \n",
      "1    -0.008385 -0.023095 -0.033498 -0.205796 -0.073056 -0.027236        0  \n",
      "2     0.006779 -0.007919 -0.043455  0.019740 -0.291624 -0.033580        0  \n",
      "3     0.005384  0.001381 -0.042828 -0.350519 -0.554685 -0.046823        0  \n",
      "4     0.025295 -0.022535 -0.035892 -0.181557 -0.027841 -0.023694        0  \n",
      "...        ...       ...       ...       ...       ...       ...      ...  \n",
      "9995  0.003007 -0.016201 -0.043479 -0.320006 -0.554635 -0.043685        0  \n",
      "9996  0.002485 -0.021760 -0.040991  0.081487  0.275468 -0.040571        0  \n",
      "9997 -0.025046 -0.009408 -0.038734 -0.250595 -0.463932 -0.040395        0  \n",
      "9998 -0.000439 -0.005170 -0.022103 -0.298745 -0.194810 -0.043671        0  \n",
      "9999  0.003478 -0.023124 -0.015060 -0.029375  0.503620 -0.017615        0  \n",
      "\n",
      "[10000 rows x 39 columns]\n",
      "['Attr2' 'Attr3' 'Attr5' 'Attr6' 'Attr9' 'Attr11' 'Attr12' 'Attr13'\n",
      " 'Attr15' 'Attr16' 'Attr17' 'Attr20' 'Attr21' 'Attr24' 'Attr25' 'Attr27'\n",
      " 'Attr29' 'Attr30' 'Attr34' 'Attr36' 'Attr37' 'Attr38' 'Attr39' 'Attr41'\n",
      " 'Attr45' 'Attr46' 'Attr47' 'Attr50' 'Attr51' 'Attr53' 'Attr54' 'Attr55'\n",
      " 'Attr57' 'Attr59' 'Attr60' 'Attr61' 'Attr63' 'Attr64' 'class_1']\n"
     ]
    }
   ],
   "source": [
    "# Creating a list of attributes to drop due to high correlation:\n",
    "#Looking at correlation between each pair of attributes, for the pairs with correlation greater than +0.80, we removed the attribute with lowest importance (importance ranked based on above analysis of means) \n",
    "attri_todrop =['Attr1','Attr7','Attr14','Attr18','Attr4','Attr7','Attr8','Attr10','Attr14','Attr18','Attr22','Attr35','Attr48','Attr26','Attr19','Attr43','Attr22','Attr23','Attr28','Attr43','Attr44','Attr62','Attr31','Attr32','Attr33','Attr48','Attr42','Attr49','Attr56','Attr40','Attr52','Attr58']\n",
    "dfsample2 = df.drop(columns=attri_todrop)\n",
    "\n",
    "# Separate the variables into numerical and categorical\n",
    "\n",
    "cat_var = ['class']\n",
    "num_var = list(dfsample2.columns.values[:-1])\n",
    "\n",
    "# Standardization of the numerical variables is not necessary since the values are already standardized\n",
    "\n",
    "# Set the correct data types\n",
    "dfsample3 = dfsample2.copy()\n",
    "dfsample3[cat_var] = dfsample2[cat_var].astype('category')\n",
    "dfsample3[num_var] = dfsample2[num_var].astype('float64')\n",
    "\n",
    "# Convert the categorical dummies\n",
    "dfsample4 = dfsample3.copy()\n",
    "dfsample4 = pd.get_dummies(dfsample3, prefix_sep='_')\n",
    "\n",
    "# Remove the redundant dummies\n",
    "# Placeholder variable: rdummies\n",
    "rdummies = ['class_0']\n",
    "dfsample5 = dfsample4.copy()\n",
    "dfsample5 = dfsample4.drop(columns=rdummies)\n",
    "\n",
    "#Printing the final result\n",
    "print(dfsample5)\n",
    "print(dfsample5.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0         1         2         3         4         5         6  \\\n",
      "0     0.000898  0.032403 -0.074542  0.040305 -1.185574  0.822187 -0.104817   \n",
      "1     0.884314  0.287615 -0.104649 -0.018772  0.714188  0.696765 -0.094750   \n",
      "2    -0.205519  0.564413 -0.845476 -0.059368  0.527698 -0.529273  0.017833   \n",
      "3    -0.160834  0.860640 -1.243781 -0.053322 -0.706462  0.439524 -0.044937   \n",
      "4    -0.774996 -1.074131  0.877776 -0.042250  0.277262 -1.504049  0.049129   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "9995  0.120888  1.067721 -1.317648 -0.058524 -0.551803  0.464714 -0.061005   \n",
      "9996  0.326805 -0.349177  0.485005 -0.022925 -1.288177  0.771557 -0.067503   \n",
      "9997  0.219769  1.357495 -1.554482 -0.054480 -0.450596  0.675223 -0.035243   \n",
      "9998 -0.125023  0.574170 -0.702234 -0.017886 -0.151898  0.712510 -0.067269   \n",
      "9999  0.316459 -0.449320  0.553262 -0.009879 -0.482924  0.841301 -0.068844   \n",
      "\n",
      "             7         8         9        10        11        12        13  \\\n",
      "0    -0.074847 -0.002193 -0.185289  0.440952  0.022478 -0.053488  0.006581   \n",
      "1    -0.092047 -0.178147 -0.074537  1.596459 -0.104219 -0.861659 -0.145909   \n",
      "2     0.004814  0.015411  0.015980 -0.352120  0.017848  0.010315 -0.074212   \n",
      "3    -0.057435  0.074304 -0.081916 -0.538714  0.094901 -0.082453 -0.112368   \n",
      "4     0.036565 -0.030257 -0.067496 -0.377460  0.058474 -0.589063 -0.115996   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "9995 -0.051020  0.044578 -0.064387 -0.208930  0.093356 -0.031663 -0.102423   \n",
      "9996 -0.063300 -0.000084 -0.103139  0.252644 -0.048676  0.491078  0.055183   \n",
      "9997 -0.045066  0.058983  0.009019 -0.364647 -0.093439  0.275658 -0.145675   \n",
      "9998 -0.053652  0.024701 -0.109064 -0.091948  0.087914 -0.254971 -0.100629   \n",
      "9999 -0.043301 -0.025386 -0.031028 -0.008350 -0.016024  0.124709 -0.003653   \n",
      "\n",
      "            14  \n",
      "0    -0.116974  \n",
      "1    -0.692461  \n",
      "2     0.035184  \n",
      "3     0.135438  \n",
      "4     0.151241  \n",
      "...        ...  \n",
      "9995  0.060955  \n",
      "9996 -0.014993  \n",
      "9997 -0.020291  \n",
      "9998  0.044551  \n",
      "9999 -0.118940  \n",
      "\n",
      "[10000 rows x 15 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       PCA1      PCA2      PCA3      PCA4      PCA5      PCA6      PCA7  class\n",
      "0  0.000898  0.032403 -0.074542  0.040305 -1.185574  0.822187 -0.104817      0\n",
      "1  0.884314  0.287615 -0.104649 -0.018772  0.714188  0.696765 -0.094750      0\n",
      "2 -0.205519  0.564413 -0.845476 -0.059368  0.527698 -0.529273  0.017833      0\n",
      "3 -0.160834  0.860640 -1.243781 -0.053322 -0.706462  0.439524 -0.044937      0\n",
      "4 -0.774996 -1.074131  0.877776 -0.042250  0.277262 -1.504049  0.049129      0\n",
      "5 -0.393465 -0.351468  0.076010 -0.058340 -0.683045 -0.753870  0.011530      0\n",
      "6 -0.323419 -0.416043  0.169594 -0.054149 -1.148218 -0.121941 -0.032954      0\n",
      "7  0.057199  1.459914 -1.382614 -0.033598  0.665326  0.963416 -0.112762      0\n",
      "8 -0.037179  1.299978 -1.559486  1.073025  0.734539 -0.075376 -0.040746      0\n",
      "9 -0.679369 -0.483721  0.163092 -0.062371  0.792733 -1.480497  0.021542      0\n",
      "PCA1     0\n",
      "PCA2     0\n",
      "PCA3     0\n",
      "PCA4     0\n",
      "PCA5     0\n",
      "PCA6     0\n",
      "PCA7     0\n",
      "class    0\n",
      "dtype: int64\n",
      "[0 1]\n",
      "          PCA1      PCA2      PCA3      PCA4      PCA5      PCA6      PCA7  \\\n",
      "0     0.000898  0.032403 -0.074542  0.040305 -1.185574  0.822187 -0.104817   \n",
      "1     0.884314  0.287615 -0.104649 -0.018772  0.714188  0.696765 -0.094750   \n",
      "2    -0.205519  0.564413 -0.845476 -0.059368  0.527698 -0.529273  0.017833   \n",
      "3    -0.160834  0.860640 -1.243781 -0.053322 -0.706462  0.439524 -0.044937   \n",
      "4    -0.774996 -1.074131  0.877776 -0.042250  0.277262 -1.504049  0.049129   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "9995  0.120888  1.067721 -1.317648 -0.058524 -0.551803  0.464714 -0.061005   \n",
      "9996  0.326805 -0.349177  0.485005 -0.022925 -1.288177  0.771557 -0.067503   \n",
      "9997  0.219769  1.357495 -1.554482 -0.054480 -0.450596  0.675223 -0.035243   \n",
      "9998 -0.125023  0.574170 -0.702234 -0.017886 -0.151898  0.712510 -0.067269   \n",
      "9999  0.316459 -0.449320  0.553262 -0.009879 -0.482924  0.841301 -0.068844   \n",
      "\n",
      "      class_1  \n",
      "0           0  \n",
      "1           0  \n",
      "2           0  \n",
      "3           0  \n",
      "4           0  \n",
      "...       ...  \n",
      "9995        0  \n",
      "9996        0  \n",
      "9997        0  \n",
      "9998        0  \n",
      "9999        0  \n",
      "\n",
      "[10000 rows x 8 columns]\n",
      "['PCA1' 'PCA2' 'PCA3' 'PCA4' 'PCA5' 'PCA6' 'PCA7' 'class_1']\n"
     ]
    }
   ],
   "source": [
    "# Data Exploration with Principal Component Analysis:\n",
    "dfpca1 = df.copy()\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_components=15)\n",
    "\n",
    "# Dropping 'class' column\n",
    "dfpca2 = dfpca1.copy()\n",
    "dfpca2 = dfpca1.drop(columns='class')\n",
    "dfpca2\n",
    "\n",
    "pca.fit(dfpca2)\n",
    "x_pca=pca.transform(dfpca2)\n",
    "x_pca.shape\n",
    "\n",
    "dfpca3 = pd.DataFrame(x_pca)\n",
    "print(dfpca3)\n",
    "\n",
    "# Plotting PCA line graph:\n",
    "per_var = np.round(pca.explained_variance_ratio_* 100, decimals=1)\n",
    "labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n",
    " \n",
    "plt.plot(range(1,len(per_var)+1), per_var)\n",
    "plt.ylabel('Percentage of Explained Variance')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.title('Scree Plot')\n",
    "plt.show()\n",
    "\n",
    "# According to line graph 1 to 7 represents complete dataset\n",
    "\n",
    "dfpca4 = dfpca3.copy()\n",
    "dfpca4.columns =[\"PCA1\",\"PCA2\",\"PCA3\",\"PCA4\",\"PCA5\",\"PCA6\",\"PCA7\",\"PCA8\",\"PCA9\",\"PCA10\",\"PCA11\",\"PCA12\",\"PCA13\",\"PCA14\",\"PCA15\"]\n",
    "dfpca4\n",
    "\n",
    "#Dropping PCA8 to PCA15\n",
    "#Here we are using 9 attributes\n",
    "\n",
    "dfpca5 = dfpca4.copy()\n",
    "todrop_pca =[\"PCA8\",\"PCA9\",\"PCA10\",\"PCA11\",\"PCA12\",\"PCA13\",\"PCA14\",\"PCA15\"]\n",
    "dfpca5 = dfpca4.drop(columns=todrop_pca)\n",
    "dfpca5\n",
    "\n",
    "# ADDING 'CLASS' column to the final data after PCA\n",
    "# dfsample5 is Final DataFrame After PCA\n",
    "\n",
    "dfpca6= dfpca5.copy()\n",
    "dfpca6['class'] = dfpca1['class']\n",
    "print(dfpca6.head(10))\n",
    "\n",
    "print(dfpca6.isnull().sum())\n",
    "\n",
    "cat_var = ['class']\n",
    "num_var = list(dfpca6.columns.values[:-1])\n",
    "\n",
    "print(dfpca6['class'].unique())\n",
    "\n",
    "# Standardization \n",
    "# Set the correct data types\n",
    "dfpca7 = dfpca6.copy()\n",
    "dfpca7[cat_var] = dfpca6[cat_var].astype('category')\n",
    "dfpca7[num_var] = dfpca6[num_var].astype('float64')\n",
    "\n",
    "# Convert the categorical dummies\n",
    "dfpca8 = dfpca7.copy()\n",
    "dfpca8 = pd.get_dummies(dfpca7, prefix_sep='_')\n",
    "\n",
    "# Remove the redundant dummies\n",
    "# Placeholder variable: rdummies\n",
    "rdummies = ['class_0']\n",
    "dfpca9 = dfpca8.copy()\n",
    "dfpca9 = dfpca8.drop(columns=rdummies)\n",
    "\n",
    "#Printing the final result\n",
    "print(dfpca9)\n",
    "print(dfpca9.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "The data set was completely made up of numerical independent variables that were standardized prioer to acquisition. The dependent variable is categorical and has been turned into dummies in the previous steps when creating the two data sets (1. Predictors selected using Mean Analysis, 2. Predictors selected using Principal Component Analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naming the data sets\n",
    "Mean_analysis = dfsample5.copy()\n",
    "Mean_analysis.name = 'Mean'\n",
    "P_C_A = dfpca9.copy()\n",
    "P_C_A.name ='PCA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Training\n",
    "\n",
    "In this section of the code, the two data frames created above will be run on a wide variety of models either being over-sampled or left as original. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all necessary libraries and functions for the iterative process of model evaluation\n",
    "\n",
    "#Models\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "\n",
    "#Scoring \n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#Utility\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.tree import export_graphviz\n",
    "from io import StringIO\n",
    "import pydotplus\n",
    "from IPython.display import Image\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADABoost Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def adaboost(train_set, test_set, treat):\n",
    "    #Start the clock of runtime\n",
    "    start_time = time.time()\n",
    "\n",
    "    #Set dependent variable placeholder\n",
    "    DV = 'class_1'\n",
    "    y = train_set[DV]\n",
    "    X = train_set.drop(columns=[DV])\n",
    "    \n",
    "    #Set placeholder variable for number of folds\n",
    "    kfolds = 5\n",
    "    \n",
    "    #Set a parameter grid for the GridSearchCV\n",
    "    param_grid = {\n",
    "                    \"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "                    \"base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "                    \"n_estimators\": [10, 50, 100, 500]\n",
    "             }\n",
    "    \n",
    "    #If statement to check whether train set needs to be oversampled\n",
    "    if treat == \"Oversample\":\n",
    "        \n",
    "        #Create a parameter grid specifically for the oversampling method.\n",
    "        param_grid = {\n",
    "                \"class__base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "                \"class__base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "                \"class__n_estimators\": [10, 50, 100, 500]\n",
    "         }\n",
    "        \n",
    "        #Assigning models and functions\n",
    "        DTC = DecisionTreeClassifier(random_state = 1, max_features = \"auto\", max_depth = 10)\n",
    "        ABC = AdaBoostClassifier(base_estimator = DTC, random_state=1)\n",
    "        smote_f = SMOTE(random_state=1)\n",
    "        \n",
    "        #Load the pipeline with the models and oversampling method\n",
    "        pipeline = Pipeline([('sampling', smote_f), ('class', ABC)])\n",
    "        \n",
    "        #Use a GridSearchCV to find the optimal model candidate\n",
    "        grid_cv = GridSearchCV(pipeline, param_grid, scoring = 'roc_auc', cv = kfolds, n_jobs=-1)\n",
    "        grid_cv.fit(X, y)\n",
    "        ABC_clf_optimal = grid_cv.best_estimator_\n",
    "    \n",
    "    if treat == \"Untreated\":\n",
    "        #Assigning models and functions\n",
    "        DTC = DecisionTreeClassifier(random_state = 1, max_features = \"auto\", max_depth = 10)\n",
    "        ABC = AdaBoostClassifier(base_estimator = DTC, random_state=1)\n",
    "        \n",
    "        #Use a GridSearchCV to find the optimal model candidate\n",
    "        grid_cv = GridSearchCV(ABC, param_grid, scoring = 'roc_auc', cv = kfolds, n_jobs=-1)\n",
    "        grid_cv.fit(X, y)\n",
    "        ABC_clf_optimal = grid_cv.best_estimator_\n",
    "        \n",
    "\n",
    "    # y_test_actual is the actual values of the DV in the test partition\n",
    "    y_test_actual = test_set[DV]\n",
    "\n",
    "    # X_test is the predictor values in the test partition\n",
    "    X_test = test_set.drop(columns=[DV])\n",
    "\n",
    "    # Get the AUC of the best LogisticRegression Classifier\n",
    "    roc_auc = roc_auc_score(y_test_actual, ABC_clf_optimal.predict_proba(X_test)[:,1])\n",
    "\n",
    "    #getting the accuracy of the best LogisticRegression Classifier\n",
    "    Accuracy_score = ABC_clf_optimal.score(X_test, y_test_actual)\n",
    "\n",
    "    #Calculating run time of program\n",
    "    Runtime = time.time() - start_time\n",
    "\n",
    "    #Setting the model name for data frame\n",
    "    model_name = 'Adaboost - Default Decision Tree'\n",
    "    \n",
    "    #return a dictionary to report the performance in a dataframe\n",
    "    return {\n",
    "        'Data Set': data_set.name,\n",
    "        'Treatment': treat,\n",
    "        'Model': model_name,\n",
    "        'Accuracy': Accuracy_score,\n",
    "        'AUC_ROC Score': roc_auc,\n",
    "        'Runtime Program': Runtime/60\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def random_forest(train_set, test_set, treat):\n",
    "    \n",
    "    #Set placeholder variable for number of folds\n",
    "    kfolds = 5\n",
    "    \n",
    "    #Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 3)]\n",
    "\n",
    "    #Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "\n",
    "    #Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num = 3)]\n",
    "    max_depth.append(None)\n",
    "\n",
    "    #Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10]\n",
    "\n",
    "    #Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "    #Create the random grid\n",
    "    param_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf}\n",
    "    \n",
    "    #Start the clock of runtime\n",
    "    start_time = time.time()\n",
    "\n",
    "    #Set dependent variable placeholder\n",
    "    DV = 'class_1'\n",
    "    y = train_set[DV]\n",
    "    X = train_set.drop(columns=[DV])\n",
    "    \n",
    "    #if statement to determine whether the train set needs oversampling or not\n",
    "    if treat == \"Oversample\":\n",
    "        \n",
    "        #Create a parameter grid specifically for the oversampling method.\n",
    "        param_grid = {'class__n_estimators': n_estimators,\n",
    "               'class__max_features': max_features,\n",
    "               'class__max_depth': max_depth,\n",
    "               'class__min_samples_split': min_samples_split,\n",
    "               'class__min_samples_leaf': min_samples_leaf}\n",
    "        \n",
    "        #Assigning models and functions\n",
    "        RFC = RandomForestClassifier(random_state=1)\n",
    "        smote_f = SMOTE(random_state=1)\n",
    "        \n",
    "        #Load the pipeline with the models and oversampling method\n",
    "        pipeline = Pipeline([('sampling', smote_f), ('class', RFC)])\n",
    "        \n",
    "        #Use a GridSearchCV to find the optimal model candidate\n",
    "        grid_cv = GridSearchCV(pipeline, param_grid, scoring = 'roc_auc', cv = kfolds, n_jobs=-1)\n",
    "        grid_cv.fit(X, y)\n",
    "        RFC_clf_optimal = grid_cv.best_estimator_\n",
    "        \n",
    "    elif treat == \"Untreated\":\n",
    "        \n",
    "        #Use a GridSearchCV to find the optimal model candidate\n",
    "        gridsearch = GridSearchCV(estimator = RandomForestClassifier(random_state=1), param_grid = param_grid, cv = kfolds, n_jobs = -1)\n",
    "        gridsearch.fit(X,y)\n",
    "        RFC_clf_optimal = gridsearch.best_estimator_\n",
    "\n",
    "    \n",
    "    # y_test_actual is the actual values of the DV in the test partition\n",
    "    y_test_actual = test_set[DV]\n",
    "\n",
    "    # X_test is the predictor values in the test partition\n",
    "    X_test = test_set.drop(columns=[DV])\n",
    "\n",
    "    # Get the AUC of the best LogisticRegression Classifier\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    roc_auc = roc_auc_score(y_test_actual, RFC_clf_optimal.predict_proba(X_test)[:,1])\n",
    "\n",
    "    #getting the accuracy of the best LogisticRegression Classifier\n",
    "    Accuracy_score = RFC_clf_optimal.score(X_test, y_test_actual)\n",
    "\n",
    "    #Calculating run time of program\n",
    "    Runtime = time.time() - start_time\n",
    "\n",
    "    #Setting the model name for data frame\n",
    "    model_name = 'Random Forest'\n",
    "\n",
    "    #return a dictionary to report the performance in a dataframe\n",
    "    return {\n",
    "        'Data Set': data_set.name,\n",
    "        'Treatment': treat,\n",
    "        'Model': model_name,\n",
    "        'Accuracy': Accuracy_score,\n",
    "        'AUC_ROC Score': roc_auc,\n",
    "        'Runtime Program': Runtime/60\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def KNN(train_set, test_set, treat):\n",
    "    \n",
    "    #Start the clock of runtime\n",
    "    start_time = time.time()\n",
    "\n",
    "    #Set dependent variable placeholder\n",
    "    DV = 'class_1'\n",
    "    y = train_set[DV]\n",
    "    X = train_set.drop(columns=[DV])\n",
    "    \n",
    "    #Set placeholder variable for number of folds\n",
    "    kfolds = 5\n",
    "    \n",
    "    #Create a parameter grid for number of neighbors\n",
    "    max_k = 200\n",
    "    \n",
    "    #Set a parameter grid for the GridSearchCV\n",
    "    param_grid = {'n_neighbors': list(range(1, max_k+1))}\n",
    "    \n",
    "    #if statement to determine whether the train set needs oversampling or not\n",
    "    if treat == \"Oversample\":\n",
    "        \n",
    "        #Create a parameter grid specifically for the oversampling method.\n",
    "        param_grid = {'class__n_neighbors': list(range(1, max_k+1))}\n",
    "\n",
    "        #Assigning models and functions\n",
    "        KNN = KNeighborsClassifier(metric='euclidean')\n",
    "        smote_f = SMOTE(random_state=1)\n",
    "    \n",
    "        #Load the pipeline with the models and oversampling method\n",
    "        pipeline = Pipeline([('sampling', smote_f), ('class', KNN)])\n",
    "\n",
    "        #Use a GridSearchCV to find the optimal model candidate\n",
    "        grid_cv = GridSearchCV(pipeline, param_grid, scoring = 'roc_auc', cv = kfolds, n_jobs=-1)\n",
    "        grid_cv.fit(X, y)\n",
    "        clf_bestKNN = grid_cv.best_estimator_\n",
    "    \n",
    "    if treat == \"Untreated\":\n",
    "        \n",
    "        #Use a GridSearchCV to find the optimal model candidate\n",
    "        gridsearch = GridSearchCV(KNeighborsClassifier(metric='euclidean'), param_grid, scoring='roc_auc', cv=kfolds, n_jobs=-1)\n",
    "        gridsearch.fit(X,y)\n",
    "        clf_bestKNN = gridsearch.best_estimator_\n",
    "\n",
    "    # y_test_actual is the actual values of the DV in the test partition\n",
    "    y_test_actual = test_set[DV]\n",
    "\n",
    "    # X_test is the predictor values in the test partition\n",
    "    X_test = test_set.drop(columns=[DV])\n",
    "\n",
    "    # Get the AUC of the best LogisticRegression Classifier\n",
    "    roc_auc = roc_auc_score(y_test_actual, clf_bestKNN.predict_proba(X_test)[:,1])\n",
    "\n",
    "    #getting the accuracy of the best LogisticRegression Classifier\n",
    "    Accuracy_score = clf_bestKNN.score(X_test, y_test_actual)\n",
    "\n",
    "    #Calculating run time of program\n",
    "    Runtime = time.time() - start_time\n",
    "\n",
    "    #Setting the model name for data frame\n",
    "    model_name = 'K-NN'\n",
    "\n",
    "    #return a dictionary to report the performance in a dataframe\n",
    "        #return a dictionary to report the performance in a dataframe\n",
    "    return {\n",
    "        'Data Set': data_set.name,\n",
    "        'Treatment': treat,\n",
    "        'Model': model_name,\n",
    "        'Accuracy': Accuracy_score,\n",
    "        'AUC_ROC Score': roc_auc,\n",
    "        'Runtime Program': Runtime/60\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Train a neural network with Cross-Validation\n",
    "\n",
    "#Run the neural networking using cross-validation and the hyperparameter grid\n",
    "def Neural_Network(df_nontest, df_test, treat):\n",
    "        \n",
    "    #Start the clock of runtime\n",
    "    start_time = time.time()\n",
    "\n",
    "    #Set dependent variable placeholder\n",
    "    DV = 'class_1'\n",
    "    y = df_nontest[DV]\n",
    "    X = df_nontest.drop(columns=[DV])\n",
    "    \n",
    "    #Placeholder kfolds\n",
    "    kfolds = 5\n",
    "\n",
    "    #The range of nodes\n",
    "    min_hls = 1\n",
    "    max_hls = 10\n",
    "\n",
    "    #Range of alpha\n",
    "    min_alpha = 0.0001\n",
    "    max_alpha = 10\n",
    "    n_alpha = 10\n",
    "    \n",
    "    #Set a parameter grid for the GridSearchCV\n",
    "    param_grid = {'hidden_layer_sizes':np.arange(min_hls, max_hls), 'alpha': list(np.linspace(min_alpha, max_alpha, num=n_alpha))}\n",
    "    \n",
    "    #if statement to determine whether the train set needs oversampling or not\n",
    "    if treat == \"Oversample\": \n",
    "        \n",
    "        #Create a parameter grid specifically for the oversampling method.\n",
    "        param_grid = {'class__hidden_layer_sizes':np.arange(min_hls, max_hls), 'class__alpha': list(np.linspace(min_alpha, max_alpha, num=n_alpha))}\n",
    "\n",
    "        #Assigning models and functions\n",
    "        NN = MLPClassifier(solver='lbfgs', max_iter=2000, random_state=1)\n",
    "        smote_f = SMOTE(random_state=1)\n",
    "\n",
    "        #Load the pipeline with the models and oversampling method\n",
    "        pipeline = Pipeline([('sampling', smote_f), ('class', NN)])\n",
    "\n",
    "        #Use a GridSearchCV to find the optimal model candidate\n",
    "        grid_cv = GridSearchCV(pipeline, param_grid, scoring = 'roc_auc', cv = kfolds, n_jobs=-1)\n",
    "        grid_cv.fit(X, y)\n",
    "        NN_clf_optimal = grid_cv.best_estimator_\n",
    "    \n",
    "    elif treat == \"Untreated\":\n",
    "        \n",
    "        #Use a GridSearchCV to find the optimal model candidate\n",
    "        gridsearch = GridSearchCV(MLPClassifier(solver='lbfgs', max_iter=2000, random_state=1), param_grid, scoring='roc_auc', cv=kfolds, n_jobs=-1)\n",
    "        gridsearch.fit(X,y)\n",
    "        NN_clf_optimal = gridsearch.best_estimator_\n",
    "\n",
    "    \n",
    "    # y_test_actual is the actual values of the DV in the test partition\n",
    "    y_test_actual = df_test[DV]\n",
    "\n",
    "    # X_test is the predictor values in the test partition\n",
    "    X_test = df_test.drop(columns=[DV])\n",
    "\n",
    "    # Get the AUC of the best LogisticRegression Classifier\n",
    "    roc_auc = roc_auc_score(y_test_actual, NN_clf_optimal.predict_proba(X_test)[:,1])\n",
    "\n",
    "    #getting the accuracy of the best LogisticRegression Classifier\n",
    "    Accuracy_score = NN_clf_optimal.score(X_test, y_test_actual)\n",
    "\n",
    "    #Calculating run time of program\n",
    "    Runtime = time.time() - start_time\n",
    "\n",
    "    #Setting the model name for data frame\n",
    "    model_name = 'Neural Network'\n",
    "\n",
    "    #return a dictionary to report the performance in a dataframe\n",
    "    return {\n",
    "        'Data Set': data_set.name,\n",
    "        'Treatment': treat,\n",
    "        'Model': model_name,\n",
    "        'Accuracy': Accuracy_score,\n",
    "        'AUC_ROC Score': roc_auc,\n",
    "        'Runtime Program': Runtime/60\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Set a function for log regression\n",
    "def log_regression(train_set, test_set, treat):\n",
    "    \n",
    "    #Set placeholder variable for number of folds\n",
    "    kfolds = 5\n",
    "\n",
    "    #Set the min and max alpha for the cross-validation to find it's optimal level between\n",
    "    min_alpha = 0.01\n",
    "    max_alpha = 100\n",
    "\n",
    "    #Generate the inverse of the min and max alpha since this is a requirement for LogisticRegressionCV\n",
    "    max_C = 1/min_alpha\n",
    "    min_C = 1/max_alpha\n",
    "\n",
    "    #Set the number of candidates\n",
    "    n_candidates = 50\n",
    "\n",
    "    #Create list to store C points in\n",
    "    C_list = list(np.linspace(min_C, max_C, num=n_candidates))\n",
    "    \n",
    "    #Start the clock of runtime\n",
    "    start_time = time.time()\n",
    "\n",
    "    #Set dependent variable placeholder\n",
    "    DV = 'class_1'\n",
    "    y = train_set[DV]\n",
    "    X = train_set.drop(columns=[DV])\n",
    "    \n",
    "    #if statement to determine whether the train set needs oversampling or not\n",
    "    if treat == \"Oversample\":\n",
    "        \n",
    "        #Load the pipeline with the models and oversampling method\n",
    "        imba_pipeline = make_pipeline(SMOTE(random_state=1), \n",
    "                                  LogisticRegression(penalty='l1', solver='saga', max_iter=1000, random_state=1))\n",
    "        \n",
    "        #Set a parameter grid for the GridSearchCV\n",
    "        param_grid = {'C': C_list}\n",
    "        #Create a parameter grid specifically for the oversampling method.\n",
    "        new_params = {'logisticregression__' + key: param_grid[key] for key in param_grid}\n",
    "        \n",
    "        #Use a GridSearchCV to find the optimal model candidate\n",
    "        grid_imba = GridSearchCV(imba_pipeline, param_grid=new_params, cv=kfolds, n_jobs=-1)\n",
    "        grid_imba.fit(X,y)\n",
    "        LOG_clf_optimal = grid_imba.best_estimator_.named_steps['logisticregression']\n",
    "\n",
    "    elif treat == \"Untreated\":\n",
    "        #Use Cross-Validation to find the optimal model candidate\n",
    "        LOG_clf_optimal = LogisticRegressionCV(Cs=C_list, cv=kfolds, scoring='roc_auc', penalty='l1', solver='saga', max_iter=1000, random_state=1, n_jobs=-1).fit(X,y)\n",
    "\n",
    "    # y_test_actual is the actual values of the DV in the test partition\n",
    "    y_test_actual = test_set[DV]\n",
    "\n",
    "    # X_test is the predictor values in the test partition\n",
    "    X_test = test_set.drop(columns=[DV])\n",
    "\n",
    "    # Get the AUC of the best LogisticRegression Classifier\n",
    "    roc_auc = roc_auc_score(y_test_actual, LOG_clf_optimal.predict_proba(X_test)[:,1])\n",
    "\n",
    "    #getting the accuracy of the best LogisticRegression Classifier\n",
    "    Accuracy_score = LOG_clf_optimal.score(X_test, y_test_actual)\n",
    "\n",
    "    #Calculating run time of program\n",
    "    Runtime = time.time() - start_time\n",
    "\n",
    "    #Setting the model name for data frame\n",
    "    model_name = 'Logistic Regression'\n",
    "\n",
    "    #return a dictionary to report the performance in a dataframe\n",
    "    return {\n",
    "        'Data Set': data_set.name,\n",
    "        'Treatment': treat,\n",
    "        'Model': model_name,\n",
    "        'Accuracy': Accuracy_score,\n",
    "        'AUC_ROC Score': roc_auc,\n",
    "        'Runtime Program': Runtime/60\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Defining DecisionTree model: \n",
    "\n",
    "# A user-defined function summary_tree to display a classification tree\n",
    "def summary_tree(model_object):\n",
    "  dot_data = StringIO()\n",
    "  export_graphviz(model_object, out_file=dot_data, filled=True,\n",
    "                  rounded=True, special_characters=True, feature_names=X.columns.values,\n",
    "                  class_names=['0', '1'])\n",
    "  graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "  output_imagefile = 'tree.png'\n",
    "  graph.write_png(output_imagefile)\n",
    "  return output_imagefile\n",
    "\n",
    "#Set Function for DecisionTree\n",
    "def decision_tree(train_set, test_set, treat):\n",
    "    #Start the clock of runtime\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #Set placeholder variable for number of folds\n",
    "    kfolds = 5\n",
    "\n",
    "    # range of depths we will search for the best pruned tree\n",
    "    maximum_depth = 100\n",
    "    minimum_depth = 1\n",
    "    \n",
    "    #Set a parameter grid for the GridSearchCV\n",
    "    param_grid = {'max_depth': list(range(minimum_depth, maximum_depth+1))}\n",
    "\n",
    "    #Set dependent variable placeholder\n",
    "    DV = 'class_1'\n",
    "    y = train_set[DV]\n",
    "    X = train_set.drop(columns=[DV])\n",
    "    \n",
    "    #if statement to determine whether the train set needs oversampling or not\n",
    "    if treat == \"Oversample\":\n",
    "        \n",
    "        #Load the pipeline with the models and oversampling method\n",
    "        imba_pipeline = make_pipeline(SMOTE(random_state=1), \n",
    "                              DecisionTreeClassifier(criterion='entropy', random_state=1))\n",
    "        \n",
    "        #Create a parameter grid specifically for the oversampling method.\n",
    "        new_params = {'decisiontreeclassifier__' + key: param_grid[key] for key in param_grid}\n",
    "        \n",
    "        #Use a GridSearchCV to find the optimal model candidate\n",
    "        grid_imba = GridSearchCV(imba_pipeline, param_grid=new_params, cv=kfolds, scoring='roc_auc', n_jobs=-1)\n",
    "        grid_imba.fit(X,y)\n",
    "        clf_BPT = grid_imba.best_estimator_.named_steps['decisiontreeclassifier']\n",
    "        \n",
    "    \n",
    "    elif treat == \"Untreated\":\n",
    "        \n",
    "        #Use a GridSearchCV to find the optimal model candidate\n",
    "        gridsearch = GridSearchCV(DecisionTreeClassifier(criterion='entropy', random_state=1), param_grid, scoring='roc_auc', cv=kfolds, n_jobs=-1)\n",
    "        gridsearch.fit(X,y)\n",
    "        clf_BPT = gridsearch.best_estimator_\n",
    "        \n",
    "    # y_test_actual is the actual values of the DV in the test partition\n",
    "    y_test_actual = test_set[DV]\n",
    "\n",
    "    # X_test is the predictor values in the test partition\n",
    "    X_test = test_set.drop(columns=[DV])\n",
    "\n",
    "    # Get the AUC of the best DecisionTree \n",
    "    roc_auc = roc_auc_score(y_test_actual, clf_BPT.predict_proba(X_test)[:,1])\n",
    "    \n",
    "\n",
    "    #getting the accuracy of the best LogisticRegression Classifier\n",
    "    Accuracy_score = clf_BPT.score(X_test, y_test_actual)\n",
    "\n",
    "    #Calculating run time of program\n",
    "    Runtime = time.time() - start_time\n",
    "\n",
    "    #Setting the model name for data frame\n",
    "    model_name = 'Decision Tree'\n",
    "\n",
    "    #return a dictionary to report the performance in a dataframe\n",
    "    return {\n",
    "        'Data Set': data_set.name,\n",
    "        'Treatment': treat,\n",
    "        'Model': model_name,\n",
    "        'Accuracy': Accuracy_score,\n",
    "        'AUC_ROC Score': roc_auc,\n",
    "        'Runtime Program': Runtime/60\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looping through all combinations\n",
    "Below is a triple nested for loop. The first loop runs over the data sets chosen by the different methods of predictor selection (Mean Analysis and PCA). The second loop runs over several different models. The third loop runs over two different options: not treating the data set for its imbalanced class or treatment with over-sampling. The results of all these loops and captured in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting with dataset\n",
      "Neural Network\n",
      "Untreated\n",
      "Oversample\n",
      "Logistic Regression\n",
      "Untreated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Untreated\n",
      "Oversample\n",
      "K-NN\n",
      "Untreated\n",
      "Oversample\n",
      "ADABoost\n",
      "Untreated\n",
      "Oversample\n",
      "starting with dataset\n",
      "Neural Network\n",
      "Untreated\n",
      "Oversample\n",
      "Logistic Regression\n",
      "Untreated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Untreated\n",
      "Oversample\n",
      "K-NN\n",
      "Untreated\n",
      "Oversample\n",
      "ADABoost\n",
      "Untreated\n",
      "Oversample\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Set</th>\n",
       "      <th>Treatment</th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC_ROC Score</th>\n",
       "      <th>Runtime Program</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mean</td>\n",
       "      <td>Untreated</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.988000</td>\n",
       "      <td>0.913173</td>\n",
       "      <td>2.669598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mean</td>\n",
       "      <td>Oversample</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.951500</td>\n",
       "      <td>0.913471</td>\n",
       "      <td>10.875496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mean</td>\n",
       "      <td>Untreated</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.835958</td>\n",
       "      <td>0.835958</td>\n",
       "      <td>0.849067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mean</td>\n",
       "      <td>Oversample</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.707500</td>\n",
       "      <td>0.837964</td>\n",
       "      <td>40.804226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mean</td>\n",
       "      <td>Untreated</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.981000</td>\n",
       "      <td>0.802712</td>\n",
       "      <td>0.369926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mean</td>\n",
       "      <td>Oversample</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.881316</td>\n",
       "      <td>1.533961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mean</td>\n",
       "      <td>Untreated</td>\n",
       "      <td>K-NN</td>\n",
       "      <td>0.982500</td>\n",
       "      <td>0.812883</td>\n",
       "      <td>3.715672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mean</td>\n",
       "      <td>Oversample</td>\n",
       "      <td>K-NN</td>\n",
       "      <td>0.666000</td>\n",
       "      <td>0.795500</td>\n",
       "      <td>6.596481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mean</td>\n",
       "      <td>Untreated</td>\n",
       "      <td>Adaboost - Default Decision Tree</td>\n",
       "      <td>0.980500</td>\n",
       "      <td>0.881170</td>\n",
       "      <td>1.525621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mean</td>\n",
       "      <td>Oversample</td>\n",
       "      <td>Adaboost - Default Decision Tree</td>\n",
       "      <td>0.975500</td>\n",
       "      <td>0.888819</td>\n",
       "      <td>3.526770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PCA</td>\n",
       "      <td>Untreated</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.982500</td>\n",
       "      <td>0.785780</td>\n",
       "      <td>0.834678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PCA</td>\n",
       "      <td>Oversample</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.668000</td>\n",
       "      <td>0.763519</td>\n",
       "      <td>5.075904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PCA</td>\n",
       "      <td>Untreated</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.756132</td>\n",
       "      <td>0.756132</td>\n",
       "      <td>0.185816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PCA</td>\n",
       "      <td>Oversample</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.629500</td>\n",
       "      <td>0.764260</td>\n",
       "      <td>9.258578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PCA</td>\n",
       "      <td>Untreated</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.981500</td>\n",
       "      <td>0.681112</td>\n",
       "      <td>0.124546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PCA</td>\n",
       "      <td>Oversample</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.468500</td>\n",
       "      <td>0.674787</td>\n",
       "      <td>0.312056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PCA</td>\n",
       "      <td>Untreated</td>\n",
       "      <td>K-NN</td>\n",
       "      <td>0.982500</td>\n",
       "      <td>0.741265</td>\n",
       "      <td>0.487532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PCA</td>\n",
       "      <td>Oversample</td>\n",
       "      <td>K-NN</td>\n",
       "      <td>0.656000</td>\n",
       "      <td>0.730273</td>\n",
       "      <td>0.741195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PCA</td>\n",
       "      <td>Untreated</td>\n",
       "      <td>Adaboost - Default Decision Tree</td>\n",
       "      <td>0.982000</td>\n",
       "      <td>0.761963</td>\n",
       "      <td>0.724693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PCA</td>\n",
       "      <td>Oversample</td>\n",
       "      <td>Adaboost - Default Decision Tree</td>\n",
       "      <td>0.935500</td>\n",
       "      <td>0.722472</td>\n",
       "      <td>1.176079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Data Set   Treatment                             Model  Accuracy  \\\n",
       "0     Mean   Untreated                    Neural Network  0.988000   \n",
       "0     Mean  Oversample                    Neural Network  0.951500   \n",
       "0     Mean   Untreated               Logistic Regression  0.835958   \n",
       "0     Mean  Oversample               Logistic Regression  0.707500   \n",
       "0     Mean   Untreated                     Decision Tree  0.981000   \n",
       "0     Mean  Oversample                     Decision Tree  0.840000   \n",
       "0     Mean   Untreated                              K-NN  0.982500   \n",
       "0     Mean  Oversample                              K-NN  0.666000   \n",
       "0     Mean   Untreated  Adaboost - Default Decision Tree  0.980500   \n",
       "0     Mean  Oversample  Adaboost - Default Decision Tree  0.975500   \n",
       "0      PCA   Untreated                    Neural Network  0.982500   \n",
       "0      PCA  Oversample                    Neural Network  0.668000   \n",
       "0      PCA   Untreated               Logistic Regression  0.756132   \n",
       "0      PCA  Oversample               Logistic Regression  0.629500   \n",
       "0      PCA   Untreated                     Decision Tree  0.981500   \n",
       "0      PCA  Oversample                     Decision Tree  0.468500   \n",
       "0      PCA   Untreated                              K-NN  0.982500   \n",
       "0      PCA  Oversample                              K-NN  0.656000   \n",
       "0      PCA   Untreated  Adaboost - Default Decision Tree  0.982000   \n",
       "0      PCA  Oversample  Adaboost - Default Decision Tree  0.935500   \n",
       "\n",
       "   AUC_ROC Score  Runtime Program  \n",
       "0       0.913173         2.669598  \n",
       "0       0.913471        10.875496  \n",
       "0       0.835958         0.849067  \n",
       "0       0.837964        40.804226  \n",
       "0       0.802712         0.369926  \n",
       "0       0.881316         1.533961  \n",
       "0       0.812883         3.715672  \n",
       "0       0.795500         6.596481  \n",
       "0       0.881170         1.525621  \n",
       "0       0.888819         3.526770  \n",
       "0       0.785780         0.834678  \n",
       "0       0.763519         5.075904  \n",
       "0       0.756132         0.185816  \n",
       "0       0.764260         9.258578  \n",
       "0       0.681112         0.124546  \n",
       "0       0.674787         0.312056  \n",
       "0       0.741265         0.487532  \n",
       "0       0.730273         0.741195  \n",
       "0       0.761963         0.724693  \n",
       "0       0.722472         1.176079  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Defining lists for the nested for loops\n",
    "data_set_list = [Mean_analysis,P_C_A]\n",
    "#model_list = ['Neural Network', 'Logistic Regression', 'Decision Tree', 'Random Forest', 'K-NN', 'ADABoost']\n",
    "\n",
    "#Create a new list to leave out Random Forest Classification since this took several hours to run and is therefore not worth it.\n",
    "model_list = ['Neural Network', 'Logistic Regression', 'Decision Tree', 'K-NN', 'ADABoost']\n",
    "treatment = ['Untreated', 'Oversample']\n",
    "\n",
    "#Creating the data frame to store the results of each data set, oversampling technique and model performance\n",
    "model_performance_df = pd.DataFrame()\n",
    "\n",
    "for data_set in data_set_list:\n",
    "    #Set variables for the data partitioning\n",
    "    df4partition = data_set\n",
    "    testpart_size = 0.2\n",
    "    \n",
    "    #Split data set in train and test\n",
    "    df_nontest, df_test = train_test_split(df4partition, test_size=testpart_size, random_state=1)\n",
    "\n",
    "    #Loop through the different models and report the corresponding results\n",
    "    print(\"starting with dataset\")\n",
    "    for model in model_list:\n",
    "        print(model)\n",
    "        for treat in treatment:\n",
    "            print(treat)\n",
    "            if model == 'Neural Network':\n",
    "                model_information = Neural_Network(df_nontest, df_test, treat)\n",
    "                model_information_df = pd.DataFrame.from_dict([model_information])\n",
    "                model_performance_df = model_performance_df.append(model_information_df)\n",
    "            elif model == 'Logistic Regression':\n",
    "                model_information = log_regression(df_nontest, df_test, treat)\n",
    "                model_information_df = pd.DataFrame.from_dict([model_information])\n",
    "                model_performance_df = model_performance_df.append(model_information_df)\n",
    "            elif model == 'Random Forest':\n",
    "                model_information = random_forest(df_nontest, df_test, treat)\n",
    "                model_information_df = pd.DataFrame.from_dict([model_information])\n",
    "                model_performance_df = model_performance_df.append(model_information_df)\n",
    "            elif model == 'Decision Tree':\n",
    "                model_information = decision_tree(df_nontest, df_test, treat)\n",
    "                model_information_df = pd.DataFrame.from_dict([model_information])\n",
    "                model_performance_df = model_performance_df.append(model_information_df)\n",
    "            elif model == 'K-NN':\n",
    "                model_information = KNN(df_nontest, df_test, treat)\n",
    "                model_information_df = pd.DataFrame.from_dict([model_information])\n",
    "                model_performance_df = model_performance_df.append(model_information_df)\n",
    "            elif model == 'ADABoost':\n",
    "                model_information = adaboost(df_nontest, df_test, treat)\n",
    "                model_information_df = pd.DataFrame.from_dict([model_information])\n",
    "                model_performance_df = model_performance_df.append(model_information_df)\n",
    "                \n",
    "\n",
    "model_performance_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
